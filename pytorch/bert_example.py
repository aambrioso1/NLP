# bert_example.py

# This example was found here:
# https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France


# Check this example later: https://pypi.org/project/pytorch-transformers/
# Then proceed with the quick tour.


from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
print(unmasker("Hello I'm a [MASK] model."))

"""
[{'sequence': "hello i'm a fashion model.", 'score': 0.10731083154678345,
 'token': 4827, 'token_str': 'fashion'}, {'sequence': "hello i'm a role model.",
  'score': 0.08774460852146149, 'token': 2535, 'token_str': 'role'}, 
  {'sequence': "hello i'm a new model.", 'score': 0.053383782505989075, 
  'token': 2047, 'token_str': 'new'}, {'sequence': "hello i'm a super model.", 
  'score': 0.04667212814092636, 'token': 3565, 'token_str': 'super'}, 
  {'sequence': "hello i'm a fine model.", 'score': 0.027095939964056015, 
  'token': 2986, 'token_str': 'fine'}]

"""

from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained("bert-base-uncased")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
print(output)
"""
BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1386,  0.1583, -0.2967,  ..., -0.2708, -0.2844,  0.4581],
         [ 0.5364, -0.2327,  0.1754,  ...,  0.5540,  0.4981, -0.0024],
         [ 0.3002, -0.3475,  0.1208,  ..., -0.4562,  0.3288,  0.8773],
         ...,
         [ 0.3799,  0.1203,  0.8283,  ..., -0.8624, -0.5957,  0.0471],
         [-0.0252, -0.7177, -0.6950,  ...,  0.0757, -0.6668, -0.3401],
         [ 0.7535,  0.2391,  0.0717,  ...,  0.2467, -0.6458, -0.3213]]],
       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.9377, -0.5043, -0.9799,  0.9030,  0.9329, -0.2438,  0.8926,  0.2288,
         -0.9531, -1.0000, -0.8862,  0.9906,  0.9855,  0.7155,  0.9455, -0.8645,
         -0.6035, -0.6666,  0.3020, -0.1587,  0.7455,  1.0000, -0.4022,  0.4261,
          0.6151,  0.9996, -0.8773,  0.9594,  0.9585,  0.6950, -0.6718,  0.3325,
         -0.9954, -0.2268, -0.9658, -0.9951,  0.6127, -0.7670,  0.0873,  0.0824,
         -0.9518,  0.4713,  1.0000,  0.3299,  0.7583, -0.2670, -1.0000,  0.3166,
         -0.9364,  0.9910,  0.9719,  0.9893,  0.2190,  0.6048,  0.5849, -0.4123,
         -0.0063,  0.1719, -0.3988, -0.6190, -0.6603,  0.5069, -0.9757, -0.9039,
          0.9926,  0.9323, -0.3687, -0.4869, -0.3143,  0.0499,  0.9129,  0.3396,
         -0.1879, -0.9235,  0.8675,  0.3228, -0.6406,  1.0000, -0.7989, -0.9931,
          0.9629,  0.9124,  0.4827, -0.7276,  0.5996, -1.0000,  0.7548, -0.1600,
         -0.9941,  0.3386,  0.8394, -0.4158,  0.2943,  0.6111, -0.5745, -0.7185,
         -0.4768, -0.9681, -0.4327, -0.6732,  0.1248, -0.2093, -0.5882, -0.4186,
          0.5447, -0.6125, -0.6138,  0.4712,  0.4779,  0.7633,  0.3974, -0.4148,
          0.7063, -0.9680,  0.7389, -0.4270, -0.9948, -0.6019, -0.9950,  0.7459,
         -0.6343, -0.2753,  0.9522, -0.5724,  0.6218, -0.1295, -0.9905, -1.0000,
         -0.8710, -0.7506, -0.5008, -0.4827, -0.9872, -0.9847,  0.7214,  0.9694,
          0.3013,  1.0000, -0.4427,  0.9699, -0.5431, -0.8189,  0.9180, -0.5132,
          0.9026,  0.5274, -0.5940,  0.2928, -0.6933,  0.7179, -0.9318, -0.2776,
         -0.9160, -0.9457, -0.3287,  0.9556, -0.7927, -0.9860, -0.1904, -0.2760,
         -0.6062,  0.9005,  0.9266,  0.4353, -0.6858,  0.4720,  0.2851,  0.7685,
         -0.8647, -0.5626,  0.5127, -0.5468, -0.9490, -0.9907, -0.5809,  0.7146,
          0.9948,  0.7981,  0.3463,  0.9349, -0.4238,  0.9333, -0.9754,  0.9936,
         -0.2597,  0.4665, -0.5400,  0.4947, -0.8723,  0.0034,  0.8378, -0.9134,
         -0.8432, -0.2516, -0.5177, -0.4687, -0.9491,  0.5691, -0.4856, -0.4857,
         -0.2245,  0.9609,  0.9823,  0.7496,  0.6256,  0.8552, -0.9073, -0.5802,
          0.2874,  0.3017,  0.3016,  0.9974, -0.8503, -0.2108, -0.9261, -0.9907,
         -0.0252, -0.9488, -0.3972, -0.8097,  0.8707, -0.7512,  0.8107,  0.5488,
         -0.9830, -0.8569,  0.4852, -0.6156,  0.4846, -0.2893,  0.9647,  0.9858,
         -0.7064,  0.7120,  0.9593, -0.9590, -0.8708,  0.7893, -0.3561,  0.8603,
         -0.7243,  0.9882,  0.9876,  0.9282, -0.9547, -0.8329, -0.7993, -0.8398,
         -0.2333,  0.2315,  0.9712,  0.6055,  0.6388,  0.2429, -0.7884,  0.9981,
         -0.9448, -0.9804, -0.8184, -0.3534, -0.9951,  0.9729,  0.4165,  0.8094,
         -0.6227, -0.8183, -0.9817,  0.8532,  0.1242,  0.9826, -0.6376, -0.9450,
         -0.8094, -0.9748,  0.0412, -0.3097, -0.8153, -0.0306, -0.9255,  0.5677,
          0.6217,  0.6652, -0.9682,  0.9997,  1.0000,  0.9826,  0.9013,  0.8950,
         -1.0000, -0.8081,  1.0000, -0.9995, -1.0000, -0.9361, -0.8200,  0.4755,
         -1.0000, -0.2698, -0.0111, -0.9297,  0.8492,  0.9879,  0.9950, -1.0000,
          0.8653,  0.9513, -0.5679,  0.9966, -0.6713,  0.9815,  0.6008,  0.7414,
         -0.3265,  0.5574, -0.9801, -0.8956, -0.8082, -0.9267,  0.9999,  0.2542,
         -0.7970, -0.8854,  0.7831, -0.1391, -0.0060, -0.9786, -0.4503,  0.8895,
          0.9021,  0.3021,  0.2650, -0.5750,  0.5099,  0.1216,  0.1170,  0.6484,
         -0.9505, -0.3889, -0.6938,  0.2508, -0.7526, -0.9831,  0.9646, -0.2742,
          0.9865,  1.0000,  0.3756, -0.9045,  0.8847,  0.4860, -0.5515,  1.0000,
          0.9092, -0.9904, -0.4959,  0.7900, -0.7156, -0.8280,  0.9999, -0.4197,
         -0.9282, -0.7733,  0.9945, -0.9956,  0.9998, -0.8985, -0.9838,  0.9735,
          0.9655, -0.8103, -0.8325,  0.1020, -0.6722,  0.4561, -0.9412,  0.8396,
          0.6979, -0.1201,  0.9288, -0.8345, -0.6312,  0.4356, -0.8901, -0.4565,
          0.9874,  0.5709, -0.2111, -0.0206, -0.4182, -0.9116, -0.9781,  0.8246,
          1.0000, -0.4229,  0.9489, -0.5226, -0.0986,  0.2202,  0.7459,  0.7152,
         -0.3528, -0.8800,  0.9299, -0.9716, -0.9949,  0.7278,  0.2206, -0.4944,
          1.0000,  0.6285,  0.3795,  0.7228,  0.9993,  0.0301,  0.5936,  0.9816,
          0.9914, -0.3465,  0.5882,  0.8365, -0.9824, -0.4488, -0.7612,  0.1331,
         -0.9479, -0.0559, -0.9697,  0.9846,  0.9960,  0.5818,  0.3121,  0.8577,
          1.0000, -0.9274,  0.6693, -0.1365,  0.8035, -1.0000, -0.8057, -0.4504,
         -0.1711, -0.9512, -0.5899,  0.3991, -0.9754,  0.9563,  0.8806, -0.9937,
         -0.9923, -0.4979,  0.8853,  0.1439, -0.9994, -0.8986, -0.6272,  0.8385,
         -0.3239, -0.9470, -0.7009, -0.4768,  0.5742, -0.2216,  0.5665,  0.9667,
          0.7935, -0.9401, -0.6746, -0.1753, -0.9163,  0.9409, -0.8701, -0.9894,
         -0.2514,  1.0000, -0.4087,  0.9385,  0.6050,  0.8219, -0.2712,  0.3326,
          0.9827,  0.3613, -0.8314, -0.9850, -0.2861, -0.5398,  0.8254,  0.8414,
          0.7590,  0.9412,  0.9627,  0.2765, -0.0737,  0.0399,  0.9998, -0.3095,
         -0.1933, -0.4689, -0.2511, -0.4629, -0.2914,  1.0000,  0.3963,  0.7777,
         -0.9950, -0.9808, -0.9303,  1.0000,  0.8822, -0.6848,  0.8124,  0.6242,
         -0.2551,  0.8266, -0.2791, -0.3167,  0.2294,  0.1682,  0.9627, -0.6738,
         -0.9904, -0.7910,  0.7099, -0.9770,  1.0000, -0.7030, -0.3960, -0.5981,
         -0.6683, -0.2727, -0.0183, -0.9882, -0.3841,  0.5605,  0.9745,  0.3505,
         -0.4898, -0.9298,  0.9578,  0.9533, -0.9859, -0.9597,  0.9777, -0.9784,
          0.7550,  1.0000,  0.3446,  0.6786,  0.3947, -0.5349,  0.5541, -0.6754,
          0.8078, -0.9595, -0.4484, -0.3901,  0.3983, -0.1319, -0.2896,  0.7860,
          0.3500, -0.5530, -0.7294, -0.2361,  0.4663,  0.9332, -0.3048, -0.1916,
          0.2318, -0.3230, -0.9323, -0.4672, -0.6315, -1.0000,  0.8068, -1.0000,
          0.8035,  0.4066, -0.3700,  0.8760,  0.7829,  0.8298, -0.8628, -0.9795,
          0.1322,  0.8529, -0.5029, -0.9057, -0.6918,  0.5017, -0.2052,  0.1564,
         -0.7397,  0.8156, -0.3414,  1.0000,  0.2659, -0.8292, -0.9821,  0.2491,
         -0.3009,  1.0000, -0.8952, -0.9832,  0.3330, -0.9180, -0.8493,  0.5868,
          0.1653, -0.8522, -0.9961,  0.9220,  0.8661, -0.6477,  0.7927, -0.3991,
         -0.7691,  0.1512,  0.9868,  0.9924,  0.7317,  0.9083, -0.1226, -0.5258,
          0.9840,  0.4009, -0.0436,  0.1361,  1.0000,  0.4004, -0.9497, -0.1309,
         -0.9788, -0.3522, -0.9551,  0.3755,  0.3099,  0.9195, -0.4460,  0.9738,
         -0.9714,  0.1901, -0.8894, -0.7863,  0.4757, -0.9463, -0.9892, -0.9938,
          0.8142, -0.4077, -0.1895,  0.2102,  0.1715,  0.6322,  0.5566, -1.0000,
          0.9642,  0.6150,  0.9768,  0.9768,  0.9115,  0.8108,  0.3251, -0.9920,
         -0.9910, -0.5438, -0.3567,  0.7960,  0.7648,  0.8900,  0.6470, -0.4875,
         -0.4792, -0.7756, -0.8423, -0.9972,  0.5961, -0.8679, -0.9678,  0.9718,
         -0.3461, -0.1534, -0.2139, -0.9586,  0.9321,  0.7627,  0.4636,  0.0862,
          0.5071,  0.9170,  0.9597,  0.9882, -0.9231,  0.8555, -0.9196,  0.6712,
          0.9381, -0.9606,  0.2335,  0.8301, -0.5560,  0.3696, -0.4752, -0.9740,
          0.8174, -0.4268,  0.7773, -0.4798,  0.0639, -0.4718, -0.2607, -0.7624,
         -0.8742,  0.6576,  0.6207,  0.9219,  0.9360, -0.0496, -0.8942, -0.3701,
         -0.8944, -0.9526,  0.9536, -0.0851, -0.2961,  0.9031,  0.1321,  0.9324,
          0.4289, -0.4989, -0.4174, -0.7639,  0.8887, -0.7894, -0.7639, -0.7093,
          0.8105,  0.3595,  1.0000, -0.9188, -0.9878, -0.8268, -0.6012,  0.4992,
         -0.7880, -1.0000,  0.3609, -0.8314,  0.8524, -0.9398,  0.9500, -0.9339,
         -0.9851, -0.3495,  0.8436,  0.9375, -0.5159, -0.8989,  0.5196, -0.8797,
          0.9979,  0.8753, -0.8277, -0.0012,  0.6013, -0.9184, -0.7398,  0.9228]],
       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
"""
from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
print(unmasker("The man worked as a [MASK]."))
"""
[{'sequence': 'the man worked as a carpenter.', 'score': 0.09747546166181564,
 'token': 10533, 'token_str': 'carpenter'}, {'sequence': 'the man worked as
  a waiter.', 'score': 0.05238321051001549, 'token': 15610, 'token_str': 'waiter'}
  , {'sequence': 'the man worked as a barber.', 'score': 0.04962707683444023, 
  'token': 13362, 'token_str': 'barber'}, 
  {'sequence': 'the man worked as a mechanic.', 'score': 0.03788604587316513, 
  'token': 15893, 'token_str': 'mechanic'}, {'sequence': 'the man worked as a 
  salesman.', 'score': 0.03768090903759003, 
  'token': 18968, 'token_str': 'salesman'}]
"""

print(unmasker("The woman worked as a [MASK]."))
"""
[{'sequence': 'the woman worked as a nurse.', 'score': 0.21981476247310638,
 'token': 6821, 'token_str': 'nurse'}, {'sequence': 'the woman worked as a 
 waitress.', 'score': 0.15974107384681702, 'token': 13877, 'token_str': 'waitress'}
  {'sequence': 'the woman worked as a maid.', 'score': 0.11547312885522842,
   'token': 10850, 'token_str': 'maid'}, {'sequence': 'the woman worked as 
   a prostitute.', 'score': 0.03796883299946785, 'token': 19215, 'token_str': 'prostitute'}, {'sequence': 'the woman worked as a cook.', 'score': 0.03042376972734928, 'token': 5660, 'token_str': 'cook'}]
"""